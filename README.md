# 📸 EmojiVision: CLIP-Powered Image Understanding

**Live Demo**: [https://emoji-vision.streamlit.app/](https://emoji-vision.streamlit.app/)  
**GitHub Repo**: [https://github.com/gbeane66/emoji-vision](https://github.com/gbeane66/emoji-vision)

## 🔍 Overview

**EmojiVision** is a playful yet powerful web app that demonstrates how multimodal AI models like OpenCLIP can understand and interpret images using natural language—or even emojis.

Users upload an image and provide three possible descriptions (including emojis). The app then uses OpenCLIP to determine which description best matches the image, displaying the result with confidence scores.

## 🚀 Features

- 🔗 **CLIP Integration**: Uses OpenCLIP (`ViT-B-32`) to compute image-text similarity.
- 🧠 **Multimodal Reasoning**: Supports both text and emoji-based descriptions.
- 📊 **Confidence Scoring**: Displays prediction probabilities and visual feedback.
- 🖼️ **Interactive UI**: Built with Streamlit for a clean, responsive user experience.
- ☁️ **Deployable**: Lightweight enough to run on Streamlit Community Cloud (CPU-only).

## 🧰 Tech Stack

- **Python**, **PyTorch**, **OpenCLIP**
- **Streamlit** for UI
- **PIL** for image handling

## 🎯 Use Cases

- **AI explainability demos** for non-technical stakeholders
- **Educational tool** for teaching vision-language models
- **Prototype** for image tagging or content moderation systems

## 🧠 What This Project Demonstrates

- Ability to integrate **state-of-the-art ML models** into production-ready apps
- Experience with **model inference**, **embedding similarity**, and **interactive UIs**
- Creativity in making AI **accessible and engaging**

<!-- ## 📸 Screenshots

*(Include a few screenshots or a GIF of the app in action)*

--- -->

Feel free to fork this repo, customize the descriptions, or extend it with new models and features!